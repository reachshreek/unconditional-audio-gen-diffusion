Modern state-of-the-art machine learning models are trained on large corpi of data. This creates a barrier to training new models for those without access to incredible training resources, and often causes models to fail to generalize outside the training set. 
Recently, the machine learning community has shown that models can be trained on just a single piece of data and learn to generate outputs very similar to the original input without the need for prompting. This results in performance superior to models trained
on large datasets while generalizing well and requiring minimal training resources. This type of model, called single-instance unconditional generation, has been used in the past for image generation, 3D asset generation, audio, and video generation. However,
the past work in single-instance unconditional audio generation used a GAN, a popular image generation method, and modified it to work with audio. Recent results have shown that a new mode of image generation, denoising diffusion probabilistic models, are 
substantially better at generation than GANs. We hypothesize that the superiority of diffusion models in generation also applies to audio, and will result in higher quality generation while still only needing a single training sample. Thus, we aim to create a 
diffusion model that can be trained on a single audio clip, and unconditionally generate similar audio. Diffusion models have been used to generate a variety of modalities such as image, video, audio, robotic behavior, and 3D assets, but as they are intended to 
generate from a large data distribution, it is challenging to modify them to learn from just one sample. Previous work has shown that diffusion models can generate unconditionally from a single 3D sample, however it has not been demonstrated that this is possible for
audio. Generating audio is far more challenging than visual artifacts because audio is both sequential (what is generated in a given segment depends on what was played before) and compositional (audio is often a combination of different instruments and voices) 
while images and 3D objects are just compositional (made of several smaller objects or parts). Thus, creating an unconditional diffusion based audio generation tool will require significant modification from previous diffusion generative models. 
